---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
I am Yuzhong Chen, an undergraduate at the Univeristy of Liverpool. My research interests include Machine Learning, Natural Language Processing, Multi-modal Large Language Model, Mixture of Experts, and AI agent development.

I am supervised by [Prof. Boris Konev](https://www.csc.liv.ac.uk/~konev/) \(Dean of School of Computer Science and Informatics, University of Liverpool\). I am in collaboration with [Asst. Prof. Qiyi Tang](https://sites.google.com/view/qiyitang/) and [Asst. Prof. Tulika Saha](https://sahatulika15.github.io) \(University of Liverpool\) over a year. Additionally, I have worked under the mentorship of [Assoc. Prof. Zengchang Qin](https://shi.buaa.edu.cn/zcqin/zh_CN/index.htm) \(Beihang University\), and Mr.Yezhou Chen, Tech Lead, Shanghai Institute of Computing Technology.


# ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CONF-SPML 2024</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An analysis of attention mechanisms and its variance in transformer](https://www.ewadirect.com/proceedings/ace/article/view/10938/pdf)

**Yuzhong Chen**, Hongren Pu, Yang Qu<sup>*</sup>

[**Project**](https://www.ewadirect.com/proceedings/ace/article/view/10938) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Analyze Transformer variants, focusing on linear complexity and sparse attention to reduce complexity, multi-head self-attention and low rank to improve performance.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Empirical Study of Modal Sequence Impact on Downstream Performance]()

**Yuzhong Chen**

[**Project**](https://github.com/tantianyuan/mllm_fine-tuning_research) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Evaluate how modality insertion order affects performance, identify interpretable patterns and propose preliminary explanations and insights for each downstream task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">FYP Project</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Modular Expert Routing vs. Unified Fine-tuning in LLM]()

**Yuzhong Chen**

[**Project**](https://github.com/Russell12138/Modular-Expert-Routing-vs.-Unified-Fine-tuning-in-LLM) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Compare both fine-tuning approaches, aiming to comprehensively assess the trade-offs between downstream performance, training cost, and inference efficiency.
</div>
</div>

# ğŸ– Honors and Awards
- *2024.10* Top 50 Excellence Award (UoL Tuition Fee Discount: Â£7000 per year) 
- *2025.06* EPSRC-funded Summer Research Assistantship, University of Liverpool

# ğŸ“– Educations
- *2024.09 - now*, Undergraduate, University of Liverpool, UK
- *2022.09 - 2024.07*, Undergraduate, Xi'an Jiaotong-Liverpool University, China

# ğŸ’» Internships
<div style="display:flex; align-items:center; width:100%;">

  <!-- å·¦ä¾§ Logo -->
  <div style="flex:1; text-align:center;">
    <img src="/assets/neurova_logo.png" alt="Neurova Logo" 
         style="max-width:120px; width:40%; border-radius:6px; box-shadow:2px 2px 6px rgba(0,0,0,0.15);">
  </div>

  <!-- å³ä¾§å†…å®¹ -->
  <div style="flex:1; padding-left:40px;">
    <strong>Neurova</strong>, New York<br>
    <em>Machine Learning Intern</em><br>
    <span style="color:#555;"><em>2025.06 â€“ 2025.08</em></span>
  </div>

</div>
