---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
I am Yuzhong Chen, an undergraduate at the Univeristy of Liverpool. My research interests include Machine Learning, Natural Language Processing, Multimodal Large Language Model, Embodied Artificial Intelligence.

I am supervised by [Prof. Boris Konev](https://www.csc.liv.ac.uk/~konev/) \(Dean of School of Computer Science and Informatics, University of Liverpool\). I am in close collaboration with [Asst. Prof. Qiyi Tang](https://sites.google.com/view/qiyitang/) and [Asst. Prof. Tulika Saha](https://sahatulika15.github.io) \(University of Liverpool\). Additionally, I have worked under the mentorship of [Assoc. Prof. Zengchang Qin](https://shi.buaa.edu.cn/zcqin/zh_CN/index.htm) \(Beihang University\), and Mr.Yezhou Chen, Tech Lead, Shanghai Institute of Computing Technology.

# üìñ Educations
- *2024.09 - now*, Undergraduate, University of Liverpool, UK
- *2022.09 - 2024.07*, Undergraduate, Xi'an Jiaotong-Liverpool University, China

# üéñ Honors and Awards
- *2024.10* Top 50 Excellence Award (UoL Tuition Fee Discount: ¬£7000 per year) 
- *2025.06* EPSRC-funded Summer Research Assistantship, University of Liverpool

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM Multimedia 2026</div><img src='images/paper3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Modular Expert Routing System in Large Language Models: Efficiency, Specialization, and Cross-Domain Generalization]()

**Yuzhong Chen**, Boris Konev<sup>*</sup>

*to be submitted*

[**Project**](https://github.com/Russell12138/Modular-Expert-Routing-vs.-Unified-Fine-tuning-in-LLM) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Compare both fine-tuning approaches, aiming to comprehensively assess the trade-offs between downstream performance, training cost, and inference efficiency.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2026</div><img src='images/paper2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Sequence Matters: A Controlled Study of Modality Injection Order in Multimodal LLMs]()

**Yuzhong Chen**, Tianyuan Tan, Aaryan Antala, Shrey Salaria, Qiyi Tang<sup>*</sup>, Tulika Saha<sup>*</sup>

*under review*

[**Project**](https://anonymous.4open.science/r/MOCA-LM-Modal-Order-Impact-64D4/README.md) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Evaluate how modality insertion order affects performance, identify interpretable patterns and propose preliminary explanations and insights for each downstream task.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CONF-SPML 2024</div><img src='images/paper1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An analysis of attention mechanisms and its variance in transformer](https://www.ewadirect.com/proceedings/ace/article/view/10938/pdf)

**Yuzhong Chen**, Hongren Pu, Yang Qu<sup>*</sup>

[**Project**](https://www.ewadirect.com/proceedings/ace/article/view/10938) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Analyze Transformer variants, focusing on linear complexity and sparse attention to reduce complexity, multi-head self-attention and low rank to improve performance.
</div>
</div>



# üìù Development

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM Multimedia 2026</div><img src='images/paper3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Modular Expert Routing System in Large Language Models: Efficiency, Specialization, and Cross-Domain Generalization]()

**Yuzhong Chen**, Boris Konev<sup>*</sup>

*to be submitted*

[**Project**](https://github.com/Russell12138/Modular-Expert-Routing-vs.-Unified-Fine-tuning-in-LLM) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Compare both fine-tuning approaches, aiming to comprehensively assess the trade-offs between downstream performance, training cost, and inference efficiency.
</div>
</div>



# üíª Internships

<div style="display:flex; align-items:center; width:100%; margin:20px 0;">

  <div style="flex:1; text-align:left;">
    <img src="images/uol_logo.png" alt="University of Liverpool Logo" 
         style="max-width:120px; width:40%; border-radius:6px; box-shadow:2px 2px 6px rgba(0,0,0,0.15);">
  </div>

  <div style="flex:1; text-align:left;">
    <strong>University of Liverpool</strong><br>
    <em>EPSRC-Funded Research Assistant</em><br>
    <span style="color:#555;"><em>2025.06 ‚Äì 2025.08</em></span>
  </div>

</div>



<div style="display:flex; align-items:center; width:100%; margin:20px 0;">

  <div style="flex:1; text-align:left;">
    <img src="images/sict_logo.jpeg" alt="SICT Logo" 
         style="max-width:120px; width:40%; border-radius:6px; box-shadow:2px 2px 6px rgba(0,0,0,0.15);">
  </div>

  <div style="flex:1; text-align:left;">
    <strong>Shanghai Institute of Computing Technology</strong><br>
    <em>Research & Development Center Intern</em><br>
    <span style="color:#555;"><em>2024.07 ‚Äì 2024.09</em></span>
  </div>

</div>
