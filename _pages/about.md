---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
I am Yuzhong Chen, an undergraduate at the Univeristy of Liverpool. My research interests include Machine Learning, Natural Language Processing, Multi-modal Large Language Model, Mixture of Experts, and AI agent development.

I am supervised by [Prof. Boris Konev](https://www.csc.liv.ac.uk/~konev/) \(Dean of School of Computer Science and Informatics, University of Liverpool\). I am in collaboration with [Asst. Prof. Qiyi Tang](https://sites.google.com/view/qiyitang/) and [Asst. Prof. Tulika Saha](https://sahatulika15.github.io) \(University of Liverpool\) over a year. Additionally, I have worked under the mentorship of [Assoc. Prof. Zengchang Qin](https://shi.buaa.edu.cn/zcqin/zh_CN/index.htm) \(Beihang University\), and Mr.Yezhou Chen, Tech Lead, Shanghai Institute of Computing Technology.


# üî• News
- *2023.10*: &nbsp;üéâüéâ First paper been accepted by CONF-SPML 2024

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CONF-SPML 2024</div><img src='images/Picture1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An analysis of attention mechanisms and its variance in transformer](https://www.ewadirect.com/proceedings/ace/article/view/10938/pdf)

**Yuzhong Chen**, Hongren Pu, Yang Qu<sup>*</sup>

[**Project**](https://www.ewadirect.com/proceedings/ace/article/view/10938) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Analyze Transformer variants, focusing on linear complexity and sparse attention to reduce complexity, multi-head self-attention and low rank to improve performance.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Empirical Study of Modal Sequence Impact on Downstream Performance]()

**Yuzhong Chen**

[**Project**](https://github.com/tantianyuan/mllm_fine-tuning_research) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Evaluate how modality insertion order affects performance, identify interpretable patterns and propose preliminary explanations and insights for each downstream task.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">FYP Project</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Modular Expert Routing vs. Unified Fine-tuning in LLM]()

**Yuzhong Chen**

[**Project**](https://github.com/Russell12138/Modular-Expert-Routing-vs.-Unified-Fine-tuning-in-LLM) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Compare both fine-tuning approaches, aiming to comprehensively assess the trade-offs between downstream performance, training cost, and inference efficiency.
</div>
</div>

# üéñ Honors and Awards
- *2024.10* Top 50 Excellence Award (UoL Tuition Fee Discount: ¬£7000 per year) 
- *2025.06* EPSRC-funded Summer Research Assistantship, University of Liverpool

# üìñ Educations
- *2024.09 - now*, Undergraduate, University of Liverpool, UK
- *2022.09 - 2024.07*, Undergraduate, Xi'an Jiaotong-Liverpool University, China

# üíª Internships
<div style="display:flex; align-items:center; width:100%;">

  <!-- Â∑¶‰æß LogoÔºåÈù†Â∑¶ -->
  <div style="flex:1; text-align:left;">
    <img src="images/logo.jpeg" alt="SICT Logo" 
         style="max-width:120px; width:40%; border-radius:6px; box-shadow:2px 2px 6px rgba(0,0,0,0.15);">
  </div>

  <!-- Âè≥‰æßÊñáÂ≠óÔºåÈù†Â∑¶ -->
  <div style="flex:1; text-align:left;">
    <strong>Shanghai Institute of Computing Technology</strong><br>
    <em>Research & Development Center Intern</em><br>
    <span style="color:#555;"><em>2024.07 ‚Äì 2024.09</em></span>
  </div>

</div>
